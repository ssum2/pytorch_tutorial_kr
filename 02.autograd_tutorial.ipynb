{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"colab":{"name":"02.autograd_tutorial.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"oyPa-dBVzNkv","colab_type":"code","colab":{}},"source":["%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9f3-Ymk3zNkz","colab_type":"text"},"source":["\n","Autograd: 자동 미분\n","===================================\n","\n","PyTorch의 모든 신경망의 중심에는 ``autograd`` 패키지가 있습니다.\n","먼저 이것을 가볍게 살펴본 뒤, 첫번째 신경망을 학습시켜보겠습니다.\n","\n","``autograd`` 패키지는 Tensor의 모든 연산에 대해 자동 미분을 제공합니다.\n","이는 실행-기반-정의(define-by-run) 프레임워크로, 이는 코드를 어떻게 작성하여\n","실행하느냐에 따라 역전파가 정의된다는 뜻이며, 역전파는 학습 과정의 매 단계마다\n","달라집니다.\n","\n","더 간단한 용어로 몇 가지 예를 살펴보겠습니다.\n","\n","Tensor\n","--------\n","\n","패키지의 중심에는 ``torch.Tensor`` 클래스가 있습니다. 만약 ``.requires_grad``\n","속성을 ``True`` 로 설정하면, 그 tensor에서 이뤄진 모든 연산들을 추적(track)하기\n","시작합니다. 계산이 완료된 후 ``.backward()`` 를 호출하여 모든 변화도(gradient)를\n","자동으로 계산할 수 있습니다. 이 Tensor의 변화도는 ``.grad`` 속성에 누적됩니다.\n","\n","Tensor가 기록을 추적하는 것을 중단하게 하려면, ``.detach()`` 를 호출하여 연산\n","기록으로부터 분리(detach)하여 이후 연산들이 추적되는 것을 방지할 수 있습니다.\n","\n","기록을 추적하는 것(과 메모리를 사용하는 것)을 방지하기 위해, 코드 블럭을\n","``with torch.no_grad():`` 로 감쌀 수 있습니다. 이는 특히 변화도(gradient)는\n","필요없지만, `requires_grad=True` 가 설정되어 학습 가능한 매개변수를 갖는 모델을\n","평가(evaluate)할 때 유용합니다.\n","\n","Autograd 구현에서 매우 중요한 클래스가 하나 더 있는데, 이것은 바로 ``Function``\n","클래스입니다.\n","\n","``Tensor`` 와 ``Function`` 은 서로 연결되어 있으며, 모든 연산 과정을\n","부호화(encode)하여 순환하지 않는 그래프(acyclic graph)를 생성합니다. 각 tensor는\n","``.grad_fn`` 속성을 갖고 있는데, 이는 ``Tensor`` 를 생성한 ``Function`` 을\n","참조하고 있습니다. (단, 사용자가 만든 Tensor는 예외로, 이 때 ``grad_fn`` 은\n","``None`` 입니다.)\n","\n","도함수를 계산하기 위해서는 ``Tensor`` 의 ``.backward()`` 를 호출하면\n","됩니다. 만약 ``Tensor`` 가 스칼라(scalar)인 경우(예. 하나의 요소 값만 갖는 등)에는\n","``backward`` 에 인자를 정해줄 필요가 없습니다. 하지만 여러 개의 요소를 갖고 있을\n","때는 tensor의 모양을 ``gradient`` 의 인자로 지정할 필요가 있습니다.\n","\n"]},{"cell_type":"code","metadata":{"id":"VOSFwYVUzNk0","colab_type":"code","colab":{}},"source":["import torch"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IcHRygyQzNk3","colab_type":"text"},"source":["tensor를 생성하고 ``requires_grad=True`` 를 설정하여 연산을 기록합니다.\n","\n"]},{"cell_type":"code","metadata":{"id":"3-pjYYNWzNk3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"35d9028d-1d20-4df2-dabd-a2d3932166c7","executionInfo":{"status":"ok","timestamp":1578749478182,"user_tz":-540,"elapsed":619,"user":{"displayName":"배수미","photoUrl":"","userId":"10423346587968401007"}}},"source":["x = torch.ones(2, 2, requires_grad=True)\n","print(x)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["tensor([[1., 1.],\n","        [1., 1.]], requires_grad=True)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sQTnG0rdzNk5","colab_type":"text"},"source":["tensor에 연산을 수행합니다:\n","\n"]},{"cell_type":"code","metadata":{"id":"-0EgVIuuzNk6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"f3d93a05-d385-4c4c-dd8b-f66a24df70c7","executionInfo":{"status":"ok","timestamp":1578749481151,"user_tz":-540,"elapsed":1120,"user":{"displayName":"배수미","photoUrl":"","userId":"10423346587968401007"}}},"source":["y = x + 2\n","print(y)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["tensor([[3., 3.],\n","        [3., 3.]], grad_fn=<AddBackward0>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8OuL5yGUzNk8","colab_type":"text"},"source":["``y`` 는 연산의 결과로 생성된 것이므로 ``grad_fn`` 을 갖습니다.\n","\n"]},{"cell_type":"code","metadata":{"id":"4i6iqjtyzNk8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"8fea47f1-de84-4ee7-ecc6-26e993ecf0fe","executionInfo":{"status":"ok","timestamp":1578749483794,"user_tz":-540,"elapsed":1131,"user":{"displayName":"배수미","photoUrl":"","userId":"10423346587968401007"}}},"source":["print(y.grad_fn)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["<AddBackward0 object at 0x7f96695c2978>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9EBwQfrlzNk-","colab_type":"text"},"source":["``y`` 에 다른 연산을 수행합니다.\n","\n"]},{"cell_type":"code","metadata":{"id":"fFSlBrQfzNk_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"2d31c998-d20f-4af4-b1b9-418e16505880","executionInfo":{"status":"ok","timestamp":1578749486497,"user_tz":-540,"elapsed":1064,"user":{"displayName":"배수미","photoUrl":"","userId":"10423346587968401007"}}},"source":["z = y * y * 3\n","out = z.mean()\n","\n","print(z, out)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["tensor([[27., 27.],\n","        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ba25cdPmzNlA","colab_type":"text"},"source":["``.requires_grad_( ... )`` 는 기존 Tensor의 ``requires_grad`` 값을 바꿔치기\n","(in-place)하여 변경합니다. 입력값이 지정되지 않으면 기본값은 ``False`` 입니다.\n","\n"]},{"cell_type":"code","metadata":{"id":"CUHykHVBzNlB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"adb6bc09-757f-42e0-8d7e-bb57d598c9c3","executionInfo":{"status":"ok","timestamp":1578749488341,"user_tz":-540,"elapsed":688,"user":{"displayName":"배수미","photoUrl":"","userId":"10423346587968401007"}}},"source":["a = torch.randn(2, 2)\n","a = ((a * 3) / (a - 1))\n","print(a.requires_grad)\n","a.requires_grad_(True)\n","print(a.requires_grad)\n","b = (a * a).sum()\n","print(b.grad_fn)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["False\n","True\n","<SumBackward0 object at 0x7f96695bf128>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Hj3R3RMozNlD","colab_type":"text"},"source":["변화도(Gradient)\n","-----------------\n","이제 역전파(backprop)를 해보겠습니다.\n","``out`` 은 하나의 스칼라 값만 갖고 있기 때문에, ``out.backward()`` 는\n","``out.backward(torch.tensor(1.))`` 과 동일합니다.\n","\n"]},{"cell_type":"code","metadata":{"id":"egEZw5SAzNlD","colab_type":"code","colab":{}},"source":["out.backward()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-OQeZH9DzNlF","colab_type":"text"},"source":["변화도 d(out)/dx를 출력합니다.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"u7ljgSvmzNlG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"6af06fbe-346e-4a0f-f2ba-b96c2b7d55b6","executionInfo":{"status":"ok","timestamp":1578749492395,"user_tz":-540,"elapsed":942,"user":{"displayName":"배수미","photoUrl":"","userId":"10423346587968401007"}}},"source":["print(x.grad)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["tensor([[4.5000, 4.5000],\n","        [4.5000, 4.5000]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Q0l38FRAzNlH","colab_type":"text"},"source":["``4.5`` 로 이루어진 행렬을 확인할 수 있습니다. ``out`` 을 *Tensor* “$o$”\n","라고 하면, 다음과 같이 구할 수 있습니다.\n","$o = \\frac{1}{4}\\sum_i z_i$ 이고,\n","$z_i = 3(x_i+2)^2$ 이므로 $z_i\\bigr\\rvert_{x_i=1} = 27$ 입니다.\n","따라서,\n","$\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)$ 이므로,\n","$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$ 입니다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"i-oFQT2pzNlI","colab_type":"text"},"source":["수학적으로 벡터 함수 $\\vec{y}=f(\\vec{x})$ 에서 $\\vec{x}$ 에\n","대한 $\\vec{y}$ 의 변화도는 야코비안 행렬(Jacobian Matrix)입니다:\n","\n","\\begin{align}J=\\left(\\begin{array}{ccc}\n","   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n","   \\vdots & \\ddots & \\vdots\\\\\n","   \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n","   \\end{array}\\right)\\end{align}\n","\n","일반적으로, ``torch.autograd`` 는 벡터-야코비안 곱을 계산하는 엔진입니다. 즉,\n","어떤 벡터 $v=\\left(\\begin{array}{cccc} v_{1} & v_{2} & \\cdots & v_{m}\\end{array}\\right)^{T}$\n","에 대해 $v^{T}\\cdot J$ 을 연산합니다. 만약 $v$ 가 스칼라 함수\n","$l=g\\left(\\vec{y}\\right)$ 의 기울기인 경우,\n","$v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}$\n","이며, 연쇄법칙(chain rule)에 따라 벡터-야코비안 곱은 $\\vec{x}$ 에 대한\n","$l$ 의 기울기가 됩니다:\n","\n","\\begin{align}J^{T}\\cdot v=\\left(\\begin{array}{ccc}\n","   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n","   \\vdots & \\ddots & \\vdots\\\\\n","   \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n","   \\end{array}\\right)\\left(\\begin{array}{c}\n","   \\frac{\\partial l}{\\partial y_{1}}\\\\\n","   \\vdots\\\\\n","   \\frac{\\partial l}{\\partial y_{m}}\n","   \\end{array}\\right)=\\left(\\begin{array}{c}\n","   \\frac{\\partial l}{\\partial x_{1}}\\\\\n","   \\vdots\\\\\n","   \\frac{\\partial l}{\\partial x_{n}}\n","   \\end{array}\\right)\\end{align}\n","\n","(여기서 $v^{T}\\cdot J$ 은 $J^{T}\\cdot v$ 를 취했을 때의 열 벡터로\n","취급할 수 있는 행 벡터를 갖습니다.)\n","\n","벡터-야코비안 곱의 이러한 특성은 스칼라가 아닌 출력을 갖는 모델에 외부 변화도를\n","제공(feed)하는 것을 매우 편리하게 해줍니다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JsT6j8qKzNlI","colab_type":"text"},"source":["이제 벡터-야코비안 곱의 예제를 살펴보도록 하겠습니다:\n","\n"]},{"cell_type":"code","metadata":{"id":"nqgzgSd-zNlJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a9a26894-2064-4d2a-bdda-9f913c7cb758","executionInfo":{"status":"ok","timestamp":1578749495589,"user_tz":-540,"elapsed":779,"user":{"displayName":"배수미","photoUrl":"","userId":"10423346587968401007"}}},"source":["x = torch.randn(3, requires_grad=True)\n","\n","y = x * 2\n","while y.data.norm() < 1000:\n","    y = y * 2\n","\n","print(y)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["tensor([ 592.0112, -584.1467,  740.9940], grad_fn=<MulBackward0>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GUe0uaDozNlL","colab_type":"text"},"source":["이 경우 ``y`` 는 더 이상 스칼라 값이 아닙니다. ``torch.autograd`` 는\n","전체 야코비안을 직접 계산할수는 없지만, 벡터-야코비안 곱은 간단히\n","``backward`` 에 해당 벡터를 인자로 제공하여 얻을 수 있습니다:\n","\n"]},{"cell_type":"code","metadata":{"id":"VWM5A3fTzNlL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"92e95fad-2a06-4fdf-a386-b02b86d69dd0","executionInfo":{"status":"ok","timestamp":1578749498539,"user_tz":-540,"elapsed":1109,"user":{"displayName":"배수미","photoUrl":"","userId":"10423346587968401007"}}},"source":["v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n","y.backward(v)\n","\n","print(x.grad)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZBwJF3JJzNlN","colab_type":"text"},"source":["또한 ``with torch.no_grad():`` 로 코드 블럭을 감싸서 autograd가\n","``.requires_grad=True`` 인 Tensor들의 연산 기록을 추적하는 것을 멈출 수 있습니다.\n","\n"]},{"cell_type":"code","metadata":{"id":"yf6pEGSqzNlO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"d0c7ce5f-3c96-4e71-af23-09bfba0641a1","executionInfo":{"status":"ok","timestamp":1578749500347,"user_tz":-540,"elapsed":1068,"user":{"displayName":"배수미","photoUrl":"","userId":"10423346587968401007"}}},"source":["print(x.requires_grad)\n","print((x ** 2).requires_grad)\n","\n","with torch.no_grad():\n","\tprint((x ** 2).requires_grad)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["True\n","True\n","False\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YTAUblPQzNlP","colab_type":"text"},"source":["**더 읽을거리:**\n","\n","``autograd.Function`` 관련 문서는 https://pytorch.org/docs/stable/autograd.html#function\n","에서 찾아볼 수 있습니다.\n","\n"]}]}